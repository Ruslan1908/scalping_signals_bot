# scalping_signals_bot/core/trainer.py
"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ AttentionLSTM –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
- –æ–±—É—á–∞–µ—Ç –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ
- —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª
"""

import torch
import torch.nn as nn
import torch.optim as optim
from models.attention_model import AttentionLSTMModel
from utils.config import DEVICE, MODEL_PATH, FEATURE_DIM, SEQ_LEN

def train_once(train_data, labels, epochs=5, lr=0.001):
    model = AttentionLSTMModel(feature_dim=FEATURE_DIM, seq_len=SEQ_LEN).to(DEVICE)
    model.train()

    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.BCEWithLogitsLoss()

    X_tensor = torch.tensor(train_data, dtype=torch.float32).to(DEVICE)
    y_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)

    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = loss_fn(outputs, y_tensor)
        loss.backward()
        optimizer.step()
        print(f"üß† –≠–ø–æ—Ö–∞ {epoch+1}/{epochs} ‚Äî Loss: {loss.item():.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"‚úÖ –í–µ—Å –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {MODEL_PATH}")

def schedule_daily_training(data_loader_func):
    X, y = data_loader_func()
    train_once(X, y)
